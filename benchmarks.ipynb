{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ffd3ec8-43fb-443f-9cba-7e70ea401931",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load dependencies - make sure to activate moeme ENV and kernel\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6506cb40-d742-400c-b96c-2cc67f7142b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'pubmedqa'...\n",
      "remote: Enumerating objects: 40, done.\u001b[K\n",
      "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
      "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
      "remote: Total 40 (delta 0), reused 1 (delta 0), pack-reused 37 (from 1)\u001b[K\n",
      "Receiving objects: 100% (40/40), 704.87 KiB | 13.55 MiB/s, done.\n",
      "Resolving deltas: 100% (12/12), done.\n"
     ]
    }
   ],
   "source": [
    "# To clone the PubMedQA Benchmark repo\n",
    "#!git clone https://github.com/pubmedqa/pubmedqa.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "794ab4c5-e618-4ab4-a365-a528703c962e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b51dc644-383a-486c-b5ab-6d9821bf054a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n",
      "Loading model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ece1a9723fe431b95154780f2881678",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Constants + Load Model\n",
    "\n",
    "# Set model path\n",
    "model_path = \"/mnt/models/MeLLaMA-13B\"\n",
    "\n",
    "# Load tokenizer\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Load model with architecture access\n",
    "print(\"Loading model...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d94e14a9-a77e-4048-ba5d-dc3ab1b7d601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark functions\n",
    "\n",
    "# Test prompt on the model\n",
    "def prompt_model(model, tokenizer, prompt, max_new_tokens=50):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Prompt function for PubMedQA\n",
    "def pubmed_prompt(model, tokenizer, pmid, data_file):\n",
    "    prompt = \"Given the QUESTION, return an ANSWER using only [yes,no,maybe].\"\n",
    "    \n",
    "    # Handle reasoning_required cases differently\n",
    "    if \"reasoning_required_pred\" in data_file[pmid] and data_file[pmid][\"reasoning_required_pred\"] == \"yes\":\n",
    "        # Join the contexts list into a single string if it's a list\n",
    "        if isinstance(data_file[pmid][\"CONTEXTS\"], list):\n",
    "            contexts = \" \".join(data_file[pmid][\"CONTEXTS\"])\n",
    "        else:\n",
    "            contexts = data_file[pmid][\"CONTEXTS\"]\n",
    "            \n",
    "        # Include context for reasoning-required questions\n",
    "        question = \"QUESTION: \" + contexts + \" \" + data_file[pmid][\"QUESTION\"]\n",
    "    else:\n",
    "        # Use only the question for non-reasoning questions\n",
    "        question = \"QUESTION: \" + data_file[pmid][\"QUESTION\"]\n",
    "    \n",
    "    # Combine prompt and question\n",
    "    full_prompt = prompt + \"\\n\\n\" + question + \"\\n\\nANSWER:\"\n",
    "    \n",
    "    # Get model output\n",
    "    full_response = prompt_model(model, tokenizer, full_prompt)\n",
    "    \n",
    "    # Extract only the generated part\n",
    "    answer_part = full_response[len(full_prompt):].strip().lower()\n",
    "    \n",
    "    # Parse to get yes, no, or maybe\n",
    "    if \"yes\" in answer_part[:20]:\n",
    "        prediction = \"yes\"\n",
    "    elif \"no\" in answer_part[:20]:\n",
    "        prediction = \"no\"\n",
    "    elif \"maybe\" in answer_part[:20]:\n",
    "        prediction = \"maybe\"\n",
    "    else:\n",
    "        # Count occurrences in case the answer isn't at the start\n",
    "        yes_count = answer_part.count(\"yes\")\n",
    "        no_count = answer_part.count(\"no\")\n",
    "        maybe_count = answer_part.count(\"maybe\")\n",
    "        \n",
    "        if yes_count > no_count and yes_count > maybe_count:\n",
    "            prediction = \"yes\"\n",
    "        elif no_count > yes_count and no_count > maybe_count:\n",
    "            prediction = \"no\"\n",
    "        else:\n",
    "            prediction = \"maybe\"\n",
    "    \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92edfd9f-03ea-4ba9-8c2e-4987ce513b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PubMedQA data from /home/dyh2111/moeme/model/pubmedqa/data/ori_pqal.json\n",
      "Starting predictions. 1000 questions found.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbc2805c3eab49218c026c45f68f6fc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run Benchmark\n",
    "\n",
    "# PubmedQA data file path\n",
    "pubmedqa_data_filepath = \"/home/dyh2111/moeme/model/pubmedqa/data/ori_pqal.json\"\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "print(f\"Loading PubMedQA data from {pubmedqa_data_filepath}\")\n",
    "with open(pubmedqa_data_filepath, 'r') as f:\n",
    "    pubmedqa_data = json.load(f)\n",
    "\n",
    "print(f\"Starting predictions. {len(pubmedqa_data.keys())} questions found.\")\n",
    "\n",
    "# Initialize predictions dictionary\n",
    "preds = {}\n",
    "\n",
    "# Record start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Process each PMID\n",
    "for pmid in tqdm(pubmedqa_data.keys()):\n",
    "    prediction = pubmed_prompt(model, tokenizer, pmid, pubmedqa_data)\n",
    "    preds[pmid] = prediction\n",
    "\n",
    "# Calculate elapsed time\n",
    "elapsed_time = (time.time() - start_time) / 60  # minutes\n",
    "\n",
    "print(\"Predictions complete\")\n",
    "print(f\"Elapsed time: {elapsed_time:.2f} mins\")\n",
    "\n",
    "# Save predictions to file\n",
    "output_file = \"mellama_pubmedqa_predictions.json\"\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(preds, f, indent=2)\n",
    "\n",
    "print(f\"Predictions saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce1b004-231c-41e7-b6c0-ce6cc3e877b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model path\n",
    "model_path = \"/mnt/models/MeLLaMA-13B\"\n",
    "\n",
    "# Load Baseline MeLLaMA-13B Model\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Load model with architecture access\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Test prompt on the model\n",
    "# Note: 4096 Max token length\n",
    "\n",
    "def prompt_model(model, tokenizer, prompt, max_new_tokens=50):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Prompt function for PubMedQA\n",
    "\n",
    "def pubmed_prompt(model,tokenizer, PMID, data_file):\n",
    "    // load json data_file\n",
    "    \n",
    "    prompt = \"Given the QUESTION, return an ANSWER using only [yes,no,maybe].\"\n",
    "\n",
    "    if data_file[PMID][\"reasoning_required_pred\"] == \"yes\":\n",
    "        question = = \"QUESTION: \" + \"data_file[PMID][\"CONTEXTS\"] + data_file[PMID][\"QUESTION\"] \n",
    "    else:\n",
    "        question = \"QUESTION: \" + data_file[PMID][\"QUESTION\"]\n",
    "    \n",
    "    inputs = tokenizer([prompt,question], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    ## filter to only yes, no maybe\n",
    "    output.lower(seach if \"yes\" == yes), no, maybe\n",
    "\n",
    "    return dict[PMID] = output\n",
    "    \n",
    "\n",
    "// PubmedQA eval location\n",
    "pubmedqa_data_filepath = \"moeme/model/pubmedqa/data/ori_pqal.json\"\n",
    "pubmedqa_data = json.load(open(pubmedqa_data_filepath)) \n",
    "\n",
    "print(\"Starting Predictions. {len(pubmedqa_data.keys} Questions Found)\n",
    "preds = dict[]\n",
    "start_time = time.now()\n",
    "for pmid in pubmedqa_data:\n",
    "    preds = pubmed_prompt(model,tokenizer, pmid, data_file)\n",
    "elapsed_time = (start_time - time.now())/60 //minutes\n",
    "print \"Predictions Complete\"\n",
    "print(\"Elapsed Time: {elapsed_time} mins\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84e6cdac-eb94-47cf-a6f6-86fc942d53a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForCausalLM\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m accuracy_score, f1_score\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'datasets'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set model path\n",
    "model_path = \"/mnt/models/MeLLaMA-13B\"\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "# Load PubMedQA dataset (labeled subset)\n",
    "dataset = load_dataset(\"pubmed_qa\", \"pqa_labeled\")[\"test\"]\n",
    "\n",
    "# Define function to extract final decision from model output\n",
    "def extract_answer(text):\n",
    "    text = text.lower()\n",
    "    for ans in [\"yes\", \"no\", \"maybe\"]:\n",
    "        if ans in text:\n",
    "            return ans\n",
    "    return \"unknown\"\n",
    "\n",
    "# Initialize lists to store predictions and references\n",
    "predictions = []\n",
    "references = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5685b122-5900-4973-9983-1a29fd24127e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ⏱️ Start timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Iterate over the dataset\n",
    "for example in tqdm(dataset):\n",
    "    question = example[\"question\"]\n",
    "    context = example[\"context\"]\n",
    "    reference = example[\"final_decision\"].lower()\n",
    "\n",
    "    # Construct the prompt\n",
    "    prompt = f\"Question: {question}\\nContext: {context}\\nAnswer:\"\n",
    "\n",
    "    # Tokenize and move to device\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=True).to(model.device)\n",
    "\n",
    "    # Generate output\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=10)\n",
    "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Extract the answer\n",
    "    prediction = extract_answer(decoded)\n",
    "\n",
    "    # Append to lists\n",
    "    predictions.append(prediction)\n",
    "    references.append(reference)\n",
    "\n",
    "# ⏱️ End timer\n",
    "end_time = time.time()\n",
    "elapsed = end_time - start_time\n",
    "\n",
    "# Calculate accuracy and macro F1 score\n",
    "accuracy = accuracy_score(references, predictions)\n",
    "macro_f1 = f1_score(references, predictions, average='macro', labels=[\"yes\", \"no\", \"maybe\"])\n",
    "\n",
    "# Print results\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Macro F1 Score: {macro_f1:.4f}\")\n",
    "print(f\"Elapsed Time: {elapsed:.2f} seconds\")\n",
    "print(f\"Avg Time per Example: {elapsed / len(dataset):.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "23ffe363-2cdc-4465-bd81-2cc8910ae3c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/dyh2111/moeme/model'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8068760d-4787-4e2e-a01a-e1b247f43ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/dyh2111/moeme/model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc60479-6b6d-4980-9829-9cf915c0bce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "moeme/model/pubmedqa/data/ori_pqal.json"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (moeme)",
   "language": "python",
   "name": "moeme"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
