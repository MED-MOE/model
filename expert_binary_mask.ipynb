{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c281a194-f738-474b-9f2f-f282815dd0bc",
   "metadata": {},
   "source": [
    "### E-LlaMA-13B Expert Creation\n",
    "### Binary Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8a35431-e599-422b-95f7-11f0dd0d707c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "#!pip install --upgrade pip\n",
    "#!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121  # For CUDA 12.1\n",
    "#!pip install transformers accelerate safetensors bitsandbytes xformers\n",
    "#!pip install scipy sentencepiece\n",
    "#!pip install ipython rich matplotlib pandas tqdm\n",
    "#sudo apt-get install gcsfuse\n",
    "#sudo apt-get update\n",
    "#sudo apt-get install fuse\n",
    "#sudo modprobe fuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135deb1c-5a87-4d5b-bce3-253ded7a5431",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2570120e-7fa4-4071-839e-6dae4dab280e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load dependencies\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "625c5d6a-f96b-45a9-a9fa-2d68cac0d2c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Connect to Bucket - only needed if copying files from bucket\n",
    "\n",
    "# Auto-mount GCS bucket on login\n",
    "#sudo mkdir -p $HOME/MeLLaMA-13B\n",
    "#sudo fusermount -u $HOME/MeLLaMA-13B 2>/dev/null\n",
    "#sudo gcsfuse --implicit-dirs dhxj_models $HOME/MeLLaMA-13B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd798e9-6a61-493f-949d-3e0bb16ba9d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfca624-ca72-4c38-a043-b90959a11314",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a865ef2-7f9e-406a-a764-2889f6d33e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ln: failed to create symbolic link '/home/dyh2111/models/models': File exists\n"
     ]
    }
   ],
   "source": [
    "# Activate MoeME env\n",
    "\n",
    "# Mount SSD to VM\n",
    "!sudo ln -s /mnt/models ~/models\n",
    "\n",
    "# Ensure models folder is visible in explorer\n",
    "!sudo ln -s /mnt/models ~/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6988524a-842c-4be2-a2c2-8dc1d5eab4cb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2024 NVIDIA Corporation\n",
      "Built on Thu_Mar_28_02:18:24_PDT_2024\n",
      "Cuda compilation tools, release 12.4, V12.4.131\n",
      "Build cuda_12.4.r12.4/compiler.34097967_0\n"
     ]
    }
   ],
   "source": [
    "#Get cuda ver - should be 12.4\n",
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c34e4549-aa1b-4f8a-addc-d19529e790e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Torch for CUDA 12.4\n",
    "#!pip3 install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9c792aa-beed-4dcc-a7db-759975bfc5f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify GPU works\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20df72b6-7369-41a5-9606-e07a1a5ec660",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: NVIDIA L4 (UUID: GPU-4b52dab4-145c-2bf0-ad48-073aa89568d2)\n"
     ]
    }
   ],
   "source": [
    "# Get GPU info\n",
    "!nvidia-smi -L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7deaae8-a727-47af-b619-1b03cfd30289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model path\n",
    "model_path = \"/mnt/models/MeLLaMA-13B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8de657ad-8301-45f9-a433-3fb2980a986a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99eac53896f647eba1a02b8fa6c14da6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "# Load Baseline MeLLaMA-13B Model\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Load model with architecture access\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f44f7856-3967-43e5-b842-4b2f1bceafa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embed_tokens.weight torch.Size([32000, 5120])\n",
      "model.layers.0.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.0.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.0.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.0.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.0.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.0.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.0.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.0.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.0.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.1.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.1.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.1.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.1.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.1.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.1.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.1.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.1.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.1.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.2.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.2.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.2.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.2.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.2.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.2.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.2.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.2.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.2.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.3.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.3.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.3.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.3.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.3.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.3.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.3.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.3.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.3.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.4.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.4.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.4.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.4.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.4.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.4.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.4.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.4.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.4.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.5.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.5.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.5.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.5.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.5.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.5.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.5.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.5.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.5.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.6.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.6.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.6.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.6.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.6.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.6.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.6.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.6.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.6.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.7.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.7.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.7.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.7.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.7.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.7.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.7.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.7.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.7.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.8.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.8.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.8.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.8.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.8.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.8.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.8.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.8.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.8.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.9.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.9.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.9.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.9.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.9.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.9.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.9.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.9.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.9.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.10.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.10.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.10.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.10.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.10.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.10.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.10.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.10.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.10.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.11.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.11.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.11.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.11.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.11.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.11.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.11.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.11.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.11.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.12.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.12.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.12.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.12.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.12.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.12.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.12.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.12.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.12.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.13.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.13.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.13.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.13.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.13.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.13.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.13.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.13.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.13.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.14.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.14.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.14.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.14.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.14.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.14.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.14.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.14.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.14.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.15.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.15.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.15.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.15.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.15.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.15.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.15.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.15.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.15.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.16.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.16.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.16.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.16.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.16.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.16.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.16.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.16.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.16.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.17.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.17.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.17.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.17.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.17.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.17.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.17.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.17.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.17.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.18.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.18.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.18.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.18.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.18.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.18.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.18.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.18.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.18.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.19.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.19.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.19.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.19.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.19.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.19.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.19.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.19.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.19.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.20.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.20.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.20.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.20.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.20.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.20.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.20.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.20.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.20.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.21.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.21.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.21.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.21.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.21.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.21.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.21.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.21.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.21.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.22.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.22.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.22.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.22.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.22.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.22.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.22.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.22.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.22.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.23.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.23.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.23.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.23.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.23.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.23.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.23.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.23.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.23.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.24.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.24.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.24.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.24.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.24.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.24.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.24.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.24.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.24.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.25.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.25.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.25.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.25.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.25.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.25.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.25.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.25.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.25.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.26.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.26.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.26.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.26.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.26.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.26.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.26.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.26.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.26.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.27.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.27.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.27.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.27.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.27.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.27.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.27.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.27.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.27.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.28.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.28.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.28.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.28.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.28.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.28.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.28.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.28.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.28.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.29.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.29.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.29.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.29.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.29.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.29.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.29.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.29.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.29.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.30.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.30.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.30.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.30.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.30.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.30.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.30.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.30.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.30.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.31.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.31.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.31.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.31.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.31.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.31.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.31.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.31.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.31.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.32.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.32.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.32.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.32.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.32.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.32.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.32.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.32.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.32.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.33.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.33.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.33.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.33.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.33.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.33.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.33.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.33.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.33.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.34.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.34.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.34.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.34.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.34.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.34.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.34.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.34.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.34.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.35.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.35.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.35.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.35.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.35.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.35.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.35.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.35.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.35.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.36.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.36.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.36.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.36.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.36.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.36.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.36.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.36.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.36.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.37.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.37.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.37.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.37.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.37.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.37.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.37.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.37.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.37.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.38.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.38.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.38.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.38.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.38.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.38.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.38.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.38.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.38.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.layers.39.self_attn.q_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.39.self_attn.k_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.39.self_attn.v_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.39.self_attn.o_proj.weight torch.Size([5120, 5120])\n",
      "model.layers.39.mlp.gate_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.39.mlp.up_proj.weight torch.Size([13824, 5120])\n",
      "model.layers.39.mlp.down_proj.weight torch.Size([5120, 13824])\n",
      "model.layers.39.input_layernorm.weight torch.Size([5120])\n",
      "model.layers.39.post_attention_layernorm.weight torch.Size([5120])\n",
      "model.norm.weight torch.Size([5120])\n",
      "lm_head.weight torch.Size([32000, 5120])\n"
     ]
    }
   ],
   "source": [
    "# View all params - prints them out\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b841852-6179-46ee-ad87-94fd244b0b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test prompt on the model\n",
    "# Note: 4096 Max token length\n",
    "\n",
    "def prompt_model(model, tokenizer, prompt, max_new_tokens=50):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8452c80a-4f3a-4c6f-aec4-ed5d9178cd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"The following are questions from the medical board exam, choose the best answer.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a0545c6-1d0b-4a27-baac-108d5c53b3ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instructions: The following are questions from the medical board exam, choose the best answer. Question: A 27-year-old woman comes to the office for counseling prior to conception. She states that a friend recently delivered a newborn with a neural tube defect and she wants to decrease her risk for having a child with this condition. She has no history of major medical illness and takes no medications. Physical examination shows no abnormalities. It is most appropriate to recommend that this patient begin supplementation with a vitamin that is a cofactor in which of the following processes?(A) Biosynthesis of nucleotides(B) Protein gamma glutamate carboxylation(C) Scavenging of free radicals(D) Transketolation(E) Triglyceride lipolysis Answer: A The recommended daily allowance (RDA) for folic acid is 400 micrograms per day for women of childbearing age. The RDA for vitamin B12 is 2.4 micro\n"
     ]
    }
   ],
   "source": [
    "# Test prompt\n",
    "\n",
    "question1 = 'A 27-year-old woman comes to the office for counseling prior to conception. She states that a friend recently delivered a newborn with a neural tube defect and she wants to decrease her risk for having a child with this condition. She has no history of major medical illness and takes no medications. Physical examination shows no abnormalities. It is most appropriate to recommend that this patient begin supplementation with a vitamin that is a cofactor in which of the following processes?\\\n",
    "(A) Biosynthesis of nucleotides\\\n",
    "(B) Protein gamma glutamate carboxylation\\\n",
    "(C) Scavenging of free radicals\\\n",
    "(D) Transketolation\\\n",
    "(E) Triglyceride lipolysis'\n",
    "\n",
    "response = prompt_model(model, tokenizer, f\"Instructions: {prompt} Question: {question1}\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ff06e71-166a-4134-8d0b-36ab269cb05f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instructions: The following are questions from the medical board exam, choose the best answer. Question: A 50-year-old man comes to the office because of a 2-month history of increasing daytime somnolence. He hasobstructive sleep apnea for which he has only intermittently used a continuous positive airway pressure device. He is170 cm (5 ft 7 in) tall and weighs 181 kg (400 lb); BMI is 63 kg/m2. His temperature is 37째C (98.6째F), pulse is 100/min,respirations are 12/min, and blood pressure is 135/80 mm Hg. Physical examination shows a gray-blue tinge to the lips,earlobes, and nail beds. Cardiac examination shows no other abnormalities. Arterial blood gas analysis on room airshows a pH of 7.31, PCO2 of 70 mm Hg, and PO2 of 50 mm Hg. Which of the following additional findings would bemost likely in this patient?\n",
      "A. Oxygen saturation of 80% on room air.\n",
      "B. Oxygen saturation of 90% on room air.\n",
      "C. Oxygen saturation of 90% on 1\n"
     ]
    }
   ],
   "source": [
    "# Test prompt\n",
    "\n",
    "question = \"A 50-year-old man comes to the office because of a 2-month history of increasing daytime somnolence. He has\\\n",
    "obstructive sleep apnea for which he has only intermittently used a continuous positive airway pressure device. He is\\\n",
    "170 cm (5 ft 7 in) tall and weighs 181 kg (400 lb); BMI is 63 kg/m2\\\n",
    ". His temperature is 37째C (98.6째F), pulse is 100/min,\\\n",
    "respirations are 12/min, and blood pressure is 135/80 mm Hg. Physical examination shows a gray-blue tinge to the lips,\\\n",
    "earlobes, and nail beds. Cardiac examination shows no other abnormalities. Arterial blood gas analysis on room air\\\n",
    "shows a pH of 7.31, PCO2 of 70 mm Hg, and PO2 of 50 mm Hg. Which of the following additional findings would be\\\n",
    "most likely in this patient?\"\n",
    "\n",
    "response = prompt_model(model, tokenizer, f\"Instructions: {prompt} Question: {question}\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "24e49a38-5f58-4acc-a07d-c6a16d219278",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "rand(): argument 'size' failed to unpack the object at pos 2 with error \"Overflow when unpacking long\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchinfo\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m summary\n\u001b[0;32m----> 3\u001b[0m \u001b[43msummary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_max_length\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtypes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m dummy_input \u001b[38;5;241m=\u001b[39m tokenizer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHello world\u001b[39m\u001b[38;5;124m\"\u001b[39m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m summary(model, input_data\u001b[38;5;241m=\u001b[39mdummy_input)\n",
      "File \u001b[0;32m~/moeme/lib/python3.10/site-packages/torchinfo/torchinfo.py:220\u001b[0m, in \u001b[0;36msummary\u001b[0;34m(model, input_size, input_data, batch_dim, cache_forward_pass, col_names, col_width, depth, device, dtypes, mode, row_settings, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m     device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(device)\n\u001b[1;32m    216\u001b[0m validate_user_params(\n\u001b[1;32m    217\u001b[0m     input_data, input_size, columns, col_width, device, dtypes, verbose\n\u001b[1;32m    218\u001b[0m )\n\u001b[0;32m--> 220\u001b[0m x, correct_input_size \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_input\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtypes\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    223\u001b[0m summary_list \u001b[38;5;241m=\u001b[39m forward_pass(\n\u001b[1;32m    224\u001b[0m     model, x, batch_dim, cache_forward_pass, device, model_mode, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    225\u001b[0m )\n\u001b[1;32m    226\u001b[0m formatting \u001b[38;5;241m=\u001b[39m FormattingOptions(depth, verbose, columns, col_width, rows)\n",
      "File \u001b[0;32m~/moeme/lib/python3.10/site-packages/torchinfo/torchinfo.py:256\u001b[0m, in \u001b[0;36mprocess_input\u001b[0;34m(input_data, input_size, batch_dim, device, dtypes)\u001b[0m\n\u001b[1;32m    254\u001b[0m         dtypes \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mfloat] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(input_size)\n\u001b[1;32m    255\u001b[0m     correct_input_size \u001b[38;5;241m=\u001b[39m get_correct_input_sizes(input_size)\n\u001b[0;32m--> 256\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mget_input_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorrect_input_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x, correct_input_size\n",
      "File \u001b[0;32m~/moeme/lib/python3.10/site-packages/torchinfo/torchinfo.py:529\u001b[0m, in \u001b[0;36mget_input_tensor\u001b[0;34m(input_size, batch_dim, dtypes, device)\u001b[0m\n\u001b[1;32m    527\u001b[0m x \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m size, dtype \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(input_size, dtypes):\n\u001b[0;32m--> 529\u001b[0m     input_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrand\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    530\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m batch_dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    531\u001b[0m         input_tensor \u001b[38;5;241m=\u001b[39m input_tensor\u001b[38;5;241m.\u001b[39munsqueeze(dim\u001b[38;5;241m=\u001b[39mbatch_dim)\n",
      "\u001b[0;31mTypeError\u001b[0m: rand(): argument 'size' failed to unpack the object at pos 2 with error \"Overflow when unpacking long\""
     ]
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(model, input_size=(1, tokenizer.model_max_length), dtypes=[torch.long])\n",
    "dummy_input = tokenizer(\"Hello world\", return_tensors=\"pt\")\n",
    "summary(model, input_data=dummy_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e668721d-db7f-4ef6-8a6c-44dcf70e05fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (moeme)",
   "language": "python",
   "name": "moeme"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
