{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c281a194-f738-474b-9f2f-f282815dd0bc",
   "metadata": {},
   "source": [
    "### E-LlaMA-13B Expert Creation\n",
    "### Binary Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a35431-e599-422b-95f7-11f0dd0d707c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "#!pip install --upgrade pip\n",
    "#!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121  # For CUDA 12.1\n",
    "#!pip install transformers accelerate safetensors bitsandbytes xformers\n",
    "#!pip install scipy sentencepiece\n",
    "#!pip install ipython rich matplotlib pandas tqdm\n",
    "#sudo apt-get install gcsfuse\n",
    "#sudo apt-get update\n",
    "#sudo apt-get install fuse\n",
    "#sudo modprobe fuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2570120e-7fa4-4071-839e-6dae4dab280e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load dependencies\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from safetensors.torch import save_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfd798e9-6a61-493f-949d-3e0bb16ba9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model path\n",
    "model_path = \"/mnt/models/MeLLaMA-13B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6988524a-842c-4be2-a2c2-8dc1d5eab4cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Get cuda ver - should be 12.4\n",
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a865ef2-7f9e-406a-a764-2889f6d33e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activate MoeME env\n",
    "\n",
    "# Mount SSD to VM\n",
    "!sudo ln -s /mnt/models ~/models\n",
    "\n",
    "# Ensure models folder is visible in explorer\n",
    "!sudo ln -s /mnt/models ~/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20df72b6-7369-41a5-9606-e07a1a5ec660",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get GPU info\n",
    "!nvidia-smi -L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34e4549-aa1b-4f8a-addc-d19529e790e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Torch for CUDA 12.4\n",
    "#!pip3 install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7deaae8-a727-47af-b619-1b03cfd30289",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de657ad-8301-45f9-a433-3fb2980a986a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eb31b29690e4462b6eb3a817ae7d34e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load Baseline MeLLaMA-13B Model to prune to CPU\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.float32)\n",
    "\n",
    "model.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44f7856-3967-43e5-b842-4b2f1bceafa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View all params - prints them out\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e668721d-db7f-4ef6-8a6c-44dcf70e05fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_ffn_layers(model, sparsity_percentage):\n",
    "    \"\"\"\n",
    "    Apply random binary mask to sparsify FFN layers of the model\n",
    "    \n",
    "    Args:\n",
    "        model: The pre-trained transformer model\n",
    "        sparsity_percentage: Float between 0 and 1 indicating percentage of nodes to remove\n",
    "    \n",
    "    Returns:\n",
    "        model: The pruned model\n",
    "    \"\"\"\n",
    "    print(f\"Starting pruning with sparsity level: {sparsity_percentage}\")\n",
    "    \n",
    "    # Count parameters before pruning\n",
    "    orig_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Original parameter count: {orig_params:,}\")\n",
    "    \n",
    "    pruned_count = 0\n",
    "    total_ffn_params = 0\n",
    "    \n",
    "    # Iterate through all modules in the model\n",
    "    for name, module in model.named_modules():\n",
    "        # Target FFN layers in transformer blocks\n",
    "        # This pattern needs to be adjusted based on your specific model architecture\n",
    "        if \"mlp\" in name.lower() or \"ffn\" in name.lower():\n",
    "            for subname, param in module.named_parameters():\n",
    "                if \"weight\" in subname:  # Focus on weight matrices\n",
    "                    total_ffn_params += param.numel()\n",
    "                    \n",
    "                    # Create binary mask (1s for keep, 0s for prune)\n",
    "                    mask = torch.rand_like(param, dtype=torch.float) > sparsity_percentage\n",
    "                    \n",
    "                    # Apply mask (hard pruning)\n",
    "                    param.data = param.data * mask.float()\n",
    "                    \n",
    "                    # Count pruned parameters\n",
    "                    pruned_count += param.numel() - mask.sum().item()\n",
    "    \n",
    "    print(f\"FFN parameters before pruning: {total_ffn_params:,}\")\n",
    "    print(f\"Parameters pruned: {pruned_count:,} ({pruned_count/orig_params:.2%} of total)\")\n",
    "    \n",
    "    # Count parameters after pruning (note: this doesn't change since we're just zeroing values)\n",
    "    remaining_params = orig_params - pruned_count\n",
    "    print(f\"Effective parameter count after pruning: {remaining_params:,}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def remove_pruned_parameters(model):\n",
    "    \"\"\"\n",
    "    Convert the pruned model (with zeroed weights) to a physically smaller model\n",
    "    This is a placeholder - actual implementation depends on model architecture\n",
    "    \"\"\"\n",
    "    # This is more complex and would require rebuilding the model architecture\n",
    "    # to physically remove the pruned nodes\n",
    "    print(\"Note: Converting masked model to physically smaller model would require\")\n",
    "    print(\"rebuilding the model architecture based on the specific transformer implementation.\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def save_pruned_model(model, output_dir, tokenizer):\n",
    "    \"\"\"\n",
    "    Save the pruned model and tokenizer\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Save model using safetensors format\n",
    "    state_dict = model.state_dict()\n",
    "    save_file(state_dict, f\"{output_dir}/model.safetensors\")\n",
    "    \n",
    "    # Save tokenizer\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    \n",
    "    print(f\"Pruned model saved to {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585f61b0-1a99-4db7-8609-ee280ac19645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "output_dir = \"pruned_model\"\n",
    "sparsity = 0.5  # 50% of FFN parameters will be pruned\n",
    "    \n",
    "# Apply pruning\n",
    "pruned_model = prune_ffn_layers(model, sparsity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826c3aa6-3dc8-4247-bf42-8be1ee603f95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (moeme)",
   "language": "python",
   "name": "moeme"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
