{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7b52a97-2f19-4b8a-9003-09a699fe1249",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed9eaf08-b0f8-40cb-a801-2715e850fa99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.508000\n",
      "Macro-F1 0.430795\n"
     ]
    }
   ],
   "source": [
    "pred_path = \"mellama_pubmedqa_predictions.json\"\n",
    "ground_truth = json.load(open('pubmedqa/data/test_ground_truth.json')) \n",
    "predictions = json.load(open(pred_path))\n",
    "\n",
    "assert set(list(ground_truth)) == set(list(predictions)), 'Please predict all and only the instances in the test set.'\n",
    "\n",
    "pmids = list(ground_truth)\n",
    "truth = [ground_truth[pmid] for pmid in pmids]\n",
    "preds = [predictions[pmid] for pmid in pmids]\n",
    "\n",
    "acc = accuracy_score(truth, preds)\n",
    "maf = f1_score(truth, preds, average='macro')\n",
    "\n",
    "print('Accuracy %f' % acc)\n",
    "print('Macro-F1 %f' % maf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2549606a-6216-476c-bace-f9d4dbbb7fe3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
