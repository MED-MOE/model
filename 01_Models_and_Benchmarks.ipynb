{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ¥ Medical Model Optimization + Mixture of Experts\n",
        "## Notebook 1\n",
        "\n",
        "**Authors**:\n",
        "\n",
        "\n",
        "*   Dan Harvey\n",
        "*   Xinzhuo Jiang\n",
        "\n",
        "\n",
        "\n",
        "**Columbia University**\n",
        "\n",
        "---\n",
        "\n",
        "In this project, we explore the optimization of Transformer-based LLM models for medical applications.  \n",
        "We aim to:\n",
        "1. Benchmark for quantitative comparion.\n",
        "2. Apply optimizations.\n",
        "3. Construct a Mixture-of-Experts (MoE) using multiple specialized models.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "QrTunYklZcC6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "kLw4YWDCYiCa"
      },
      "outputs": [],
      "source": [
        "## ðŸ“¦ Environment Setup: Dependencies and Imports\n",
        "import os\n",
        "import sys\n",
        "import importlib\n",
        "import subprocess\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add project root to path\n",
        "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
        "if project_root not in sys.path:\n",
        "    sys.path.append(project_root)\n",
        "\n",
        "# Required packages\n",
        "required_packages = [\n",
        "    'torch', 'transformers', 'datasets', 'accelerate', 'flash_attn',\n",
        "    'evaluate', 'lm_eval', 'sklearn', 'matplotlib', 'wandb',\n",
        "    'tqdm', 'sentencepiece', 'scipy', 'einops'\n",
        "]\n",
        "\n",
        "# Check and install missing packages\n",
        "for package in required_packages:\n",
        "    try:\n",
        "        module = importlib.import_module(package)\n",
        "        print(f\"âœ… {package} installed successfully\")\n",
        "        if package == 'torch':\n",
        "            print(f\"   Version: {torch.__version__}\")\n",
        "            print(f\"   CUDA available: {torch.cuda.is_available()}\")\n",
        "            if torch.cuda.is_available():\n",
        "                print(f\"   CUDA version: {torch.version.cuda}\")\n",
        "                print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
        "        elif hasattr(module, '__version__'):\n",
        "            print(f\"   Version: {module.__version__}\")\n",
        "    except ImportError:\n",
        "        print(f\"âŒ {package} not found. Installing...\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
        "        module = importlib.import_module(package)\n",
        "        print(f\"âœ… {package} installed successfully (post-install)\")\n",
        "        if hasattr(module, '__version__'):\n",
        "            print(f\"   Version: {module.__version__}\")\n",
        "\n",
        "# You may need to restart the Kernel to use these"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLIs-kp2ahS5",
        "outputId": "79d4ff5a-b82d-4cf9-976f-a1eceb937d46"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… torch installed successfully\n",
            "   Version: 2.6.0+cu124\n",
            "   CUDA available: False\n",
            "âœ… transformers installed successfully\n",
            "   Version: 4.51.3\n",
            "âŒ datasets not found. Installing...\n",
            "âœ… datasets installed successfully (post-install)\n",
            "   Version: 3.5.1\n",
            "âœ… accelerate installed successfully\n",
            "   Version: 1.6.0\n",
            "âŒ flash_attn not found. Installing...\n",
            "âœ… flash_attn installed successfully (post-install)\n",
            "   Version: 2.7.4.post1\n",
            "âŒ evaluate not found. Installing...\n",
            "âœ… evaluate installed successfully (post-install)\n",
            "   Version: 0.4.3\n",
            "âŒ lm_eval not found. Installing...\n",
            "âœ… lm_eval installed successfully (post-install)\n",
            "âœ… sklearn installed successfully\n",
            "   Version: 1.6.1\n",
            "âœ… matplotlib installed successfully\n",
            "   Version: 3.10.0\n",
            "âœ… wandb installed successfully\n",
            "   Version: 0.19.10\n",
            "âœ… tqdm installed successfully\n",
            "   Version: 4.67.1\n",
            "âœ… sentencepiece installed successfully\n",
            "   Version: 0.2.0\n",
            "âœ… scipy installed successfully\n",
            "   Version: 1.15.2\n",
            "âœ… einops installed successfully\n",
            "   Version: 0.8.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "30lAxenGa2MG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "oRmkvbtZa7Ya"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xurL4_uRayhx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ§  Model Selection: Baseline Models\n",
        "\n",
        "We will work with the following Hugging Face models:\n",
        "\n",
        "| Model Name                            | Size | Notes                                                          |\n",
        "| ------------------------------------- | ---- | -------------------------------------------------------------- |\n",
        "| `TsinghuaC3I/Llama-3-8B-UltraMedical` | 8B   | Medical domain-specific, fine-tuned, ideal teacher & benchmark |\n",
        "| `meta-llama/Llama-3.2-3B`             | 3B   | Same architecture, smaller, ideal as an expert or student      |\n",
        "| `Qwen/Qwen3-4B`                       | 4B   | Non-LLaMA expert for diversity in MoE                          |\n",
        "\n",
        "These models will serve as the baseline in our pipeline and will be evaluated for:\n",
        "\n",
        "- Performance on medical QA and reasoning tasks\n",
        "- Suitability for distillation and expert specialization\n",
        "- Impact of downstream optimizations (quantization, pruning, MoE routing)\n",
        "\n",
        "ðŸ“Œ **Note**: All models are initially loaded in **full FP32 (float32) precision** to serve as accurate performance baselines before applying any quantization or memory optimization techniques.\n"
      ],
      "metadata": {
        "id": "Tc4-MJc6a1jz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ðŸ” Hugging Face Access - Llama is Gated\n",
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "ZDoqAHm2bC8D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ“¥ Load Baseline Models"
      ],
      "metadata": {
        "id": "-9XVHTJxbR7I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load section dependencies\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM"
      ],
      "metadata": {
        "id": "Fgi10syEcUIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ¦™ Llama-3-8B-UltraMedical\n",
        "\n",
        "**Links**  \n",
        "- ðŸ¤— [Hugging Face Model Card](https://huggingface.co/TsinghuaC3I/Llama-3-8B-UltraMedical)\n",
        "- ðŸ“„ [Paper / Source](https://huggingface.co/TsinghuaC3I/Llama-3-8B-UltraMedical)\n",
        "\n",
        "**Approximate GPU Memory Requirements:**\n",
        "- **FP32**: ~95 GB  \n",
        "- **FP16**: ~48 GB  \n",
        "- **INT8**: ~24 GB  \n",
        "- **INT4**: ~12 GB  \n",
        "\n",
        "> These values are estimates and may vary based on sequence length, attention optimizations, and tokenizer overhead.\n"
      ],
      "metadata": {
        "id": "rPQOG84wb3_k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Llama-3-8B-UltraMedical\n",
        "\n",
        "tokenizer_llama8b_med = AutoTokenizer.from_pretrained(\n",
        "    \"TsinghuaC3I/Llama-3-8B-UltraMedical\",\n",
        "    trust_remote_code=True,\n",
        "    use_auth_token=True\n",
        ")\n",
        "\n",
        "model_llama8b_med = AutoModelForCausalLM.from_pretrained(\n",
        "    \"TsinghuaC3I/Llama-3-8B-UltraMedical\",\n",
        "    trust_remote_code=True,\n",
        "  device_map=\"auto\",\n",
        "    torch_dtype=torch.float32,\n",
        "    use_auth_token=True\n",
        ")\n",
        "\n",
        "print(\"âœ… Loaded Llama-3-8B-UltraMedical (FP32, device-mapped)\")"
      ],
      "metadata": {
        "id": "T2k90mbJcOW3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ¦™ Llama-3.2-3B\n",
        "\n",
        "**Links**  \n",
        "- ðŸ¤— [Hugging Face Model Card](https://huggingface.co/meta-llama/Llama-3.2-3B)  \n",
        "- ðŸ“„ [Paper / Source](https://huggingface.co/meta-llama/Llama-3.2-3B)\n",
        "\n",
        "**Approximate GPU Memory Requirements:**\n",
        "- **FP32**: ~36 GB  \n",
        "- **FP16**: ~18 GB  \n",
        "- **INT8**: ~9 GB  \n",
        "- **INT4**: ~4.5 GB  \n",
        "\n",
        "> These are rough estimates. Actual usage depends on sequence length, architecture-specific memory optimizations, and tokenizer overhead.\n"
      ],
      "metadata": {
        "id": "DsEqZ9NHce22"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Llama-3.2-3B\n",
        "\n",
        "tokenizer_llama3b = AutoTokenizer.from_pretrained(\n",
        "    \"meta-llama/Llama-3.2-3B\",\n",
        "    trust_remote_code=True,\n",
        "    use_auth_token=True\n",
        ")\n",
        "\n",
        "model_llama3b = AutoModelForCausalLM.from_pretrained(\n",
        "    \"meta-llama/Llama-3.2-3B\",\n",
        "    trust_remote_code=True,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float32,\n",
        "    use_auth_token=True\n",
        ")\n",
        "\n",
        "print(\"âœ… Loaded Llama-3.2-3B (FP32, device-mapped)\")\n"
      ],
      "metadata": {
        "id": "nU7Om9Npck6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ‰ Qwen3-4B\n",
        "\n",
        "**Links**  \n",
        "- ðŸ¤— [Hugging Face Model Card](https://huggingface.co/Qwen/Qwen3-4B)  \n",
        "- ðŸ“„ [Paper / Source](https://arxiv.org/abs/2403.08552) *(Qwen2 paper for reference â€” Qwen3 paper may be pending)*\n",
        "\n",
        "**Approximate GPU Memory Requirements:**\n",
        "- **FP32**: ~48 GB  \n",
        "- **FP16**: ~24 GB  \n",
        "- **INT8**: ~12 GB  \n",
        "- **INT4**: ~6 GB  \n",
        "\n",
        "> Qwen models typically require `trust_remote_code=True` due to custom model implementations."
      ],
      "metadata": {
        "id": "R6oACZ6gcuUm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Qwen3-4B\n",
        "\n",
        "tokenizer_qwen4b = AutoTokenizer.from_pretrained(\n",
        "    \"Qwen/Qwen3-4B\",\n",
        "    trust_remote_code=True,\n",
        "    use_auth_token=True\n",
        ")\n",
        "\n",
        "model_qwen4b = AutoModelForCausalLM.from_pretrained(\n",
        "    \"Qwen/Qwen3-4B\",\n",
        "    trust_remote_code=True,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float32,\n",
        "    use_auth_token=True\n",
        ")\n",
        "\n",
        "print(\"âœ… Loaded Qwen3-4B (FP32, device-mapped)\")\n"
      ],
      "metadata": {
        "id": "bZkjduGzcucu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ“Š Benchmarking\n",
        "\n",
        "To establish performance baselines, we will:\n",
        "\n",
        "* Load eeach model in full float32 (Already implemented above)\n",
        "* Run each model through standard medical QA tasks (e.gPubMedQA).\n",
        "* Repeat each benchmark 3 times and average results.\n"
      ],
      "metadata": {
        "id": "xN5AYSOubW30"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ðŸ“Š Benchmarking\n",
        "\n",
        "To establish performance baselines, we will:\n",
        "\n",
        "Run each model through standard medical QA tasks (e.g., PubMedQA).\n",
        "Repeat each benchmark 3 times and average results.\n",
        "Benchmarking code to be added here."
      ],
      "metadata": {
        "id": "L3OSTigybW_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "IeFwT-wWffKS"
      }
    }
  ]
}